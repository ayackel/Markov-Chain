Note this was only one of the points of several.  My concern of Jira is formality of discussion that may not be necessary and having philosophical debates/differences in that medium.  Also, that is harder / less effective to be communicating ‘in print’ in that forum and sometimes tone or similar can be misinterpreted.
I want to be careful of general picking at the existing architecture/design that is there intentionally and discussion before this can be distracting and not scalable to attend to with every new person that has a different desire for doing something and to not spin in Jira for things that are not determined applicable for consideration.
Ok, but this logistic was on our list for he/I to be discussing more proactively.  I’d like to ensure we keep to that dynamic.

This item is invasive to a design strategy chosen w/reason.  I understand and appreciate the intent, but my initial thoughts are that I do not agree there is any concerning issue with velocity or complexity here, it’s not anything directly customer impacting, current strategy is a well understood development pattern we are used to, using child exceptions is not uncommon and its use here is simplistic, it ripples and aligns well with how we map to an abstraction of public facing error keys, and is not a value distraction compared to other more meaningful work we are trying to get to.  I’d prefer to not have too much spin here discounting these items.

This is the kind of change to architecture/design strategy we were discussing to sync with me first before just pursuing.  We want to discuss these before backlog items that may not be agreed as appropriate.

I can think about this one item in particular – not saying yet either way here- just a reminder in general.

Ok.  So the EMPTY_DOCUMENT behavior should be consistent then/now/moving-forward.  This would be relevant to the empty case (not the 4 newline case).  You might want to check the engine side logs to see if ENGINE_FAILED_PROCESSING was due to an exception the engine or the skipped documents (the log will be clear about both).  Also another thought is looking at the untagged xhtml that is created as part of PFJ that should be an indication of content, or the inbound message XML that shows the document text passed to the engine.
I would err on the aspect of application / customer facing behavior over the  developer/code logistic, which can be managed through TODOs at the dependent code points, which need to be there anyway given its temporary scope, but it's temporary so either way is fine.
It's an option, but it brings in expanded consideration of transaction windows, deadlock, increased DB activity across all thread activity.  Likely these things will be low risk, but the document  table gets a lot of concurrent activity request/response.
Normally this kind of data check would be either part of the first step of the workflow service call or more recently the validation framework (this is a good example of data in hand check).

The question of bug is working as designed as implemented and it’s not a problem that the document has been persisted for document-based workflows and should not affect QA or the release.  Moving forward we’d probably prefer this kind of check higher up.  Perhaps a backlog  item to move this to validation.

It’s not intended that way, but the position is applicable because IMO these things are not accurate.  I do not want to have to  replicate in the context of this thread the lengthy examples and data that has already been communicated previously in this regard at the risk of bloating or distracting this thread.  I have raised similarly in the past these ‘defenses’ have been out there and qualify every discussion, but glossed over.  I don’t want to be repeating here.  The idea of this thread is to be a step back from those details and have some summarized take-aways to be more conscious of moving forward.
] I’m aware of, appreciate, and sense these attributes, which is why I mentioned we could be powerful together if we can move forward on some of the dynamics here being raised.  You’re still gaining needed breadth/depth in many areas with this still being a fairly new position, so not being at that level just yet is understandable, and I see it around the horizon.  Keep in mind your role is not redirecting all these touch points to re-engineer them how you envision them better or different in your eyes, especially when conflicting with those aspects that are in place with reason and that have similarly been successful.  There are other ways and philosophies of achieving the same or better success.  But there is mutual benefit from experiences here.  We just have to be careful of the frequency and approach of trying to change these things at risk of being invasive, distracting, or not aligning with priority, especially before you’re fully up to  speed.  It especially affects my cycles to have to stop what I’m doing to give full (detailed ;) ) attention this regularly.  We can work together to be effective with this dynamic.  
I would hope you can also just respect at a higher level these items that are shared without having to do a code review of them (exaggeration, slightly).  This very much could bog us down.  Sometimes there are intangible benefits in the list that may not have a hard code change to look at it, so we need to be able to appreciate/trust these things at a conceptual level.  I can go into deeper detail of those specific things, but I honestly don’t see how the point can’t be appreciated without this level of detail.  Communicating being able to update an XML wiring in place should be sufficient without looking at a specific bean parameter.  
They were heavily communicated in the initial thread and again a significant subset in the 2nd thread.  As I said, it may not be that all people will have to be on board.  This is not always a vote system or who you can  get to back up a desired change to challenge.  Sometimes there are tradeoffs/ factors again that may not be appreciated (even when reviewed), but that doesn’t invalidate them, and sometimes they have to be trumping.
I think this is a distracting detail (no pun intended ;) ) to some general high level items to have harder awareness.  See above that we need to be able to communicate and appreciate at a conceptual level, if even with some high level of detail backing it.  The needs are communicated and should be sufficient at some conceptual level – for example we wanted to be able to  have externalized validation enablement even at a per validation item level since we might have a critical blocking issue with one element, but not want to lose the integrity enforced by the rest (this was noted from the beginning of the validation work).  This direct customer benefit that really should be halting factor to that debate.  Another unrelated intangible XML in general benefit (w/Spring) I gave was to the ability to share application pieces like this to outside consumers, such as an ActiveMQ consultant that knows AMQs spring integration and would be able to ramp easily to our applications use of the technology simply by reviewing the XML interface describing the artifacts wired there and centrally viewable – no code review needed or feasible.  I don’t see how more details beyond this are blocking to appreciate that dynamic.  There are a lot of other details and backing from a lot of threads overlapping the points I’m raising below.  I’d rather not have to walk through them all again and hope that they can be better appreciated/respected at the time of communication.  

Unfortunately I don’t sense we’ve broken ground here or will be moving forward, but hopefully at some level you appreciate the items below that I’ve struggled with and are impacting my contribution levels, but we can keep working on it given the potential on the horizon as these come together.  Thanks for the ear.

That’s the point.  It did not.  There were intangible dynamics why XML was preferred, some of which included flexibility for customer facing problem resolution, with some optional cards that could be played beyond the coarse all on/off.  The logistic was more and never just about whether there was practical solution to using annotations.  The XML argument was not based on this.  I know Will would lean to annotations here as a developer convenience, but that does not change the XML value points that are trumping. 
Quite the opposite.  My responses are lengthy with an extreme number of items with rationale and justification and often  with hard examples backing them.  The issue is you gloss over them and disregard them.  I qualify with many bigger picture dynamics that you are still looking to get stronger with to be diligent to make a more complete argument.  We’ve talked a lot for example with just Spring in your interview, architecture overviews, and sporadic related email debates.  The volume on these things is high.  They key is that things need to be championed even if you have a different philosophy or don’t agree.  Don’t confuse not appreciating these points with not having the data.
We are making the same point on both sides.  As noted ‘team’ is over-generalized.  There is concurrence with supportive views on the positions with any of the debates.  And, again keep the context that your marketing of things is more narrowed to the developer circle, which people may gravitate too, but without the additional context of the alternate tradeoffs opinions will be made prematurely or without adequate data.  Experience shows as noted the skill for developers to expand their skill set with appreciating the intangible dynamics outside of development is one many struggle with.  I’ve made a lot of progress with ‘the team’ in strengthening this aspect.  Some of the changes being proposed compromise that.
I agree this is a gap I want to help  make stronger for you, which is case in point to  what we talked about on-site to often lean to what I’m trying to advocate because the tradeoffs backing them are  a bit more extensive with much more breadth/depth experience in each individual’s strengths and weaknesses, the application code base against all CLU products, all CLU products etc.  This relates to the comments above with the data being provided, but glossed over.  You need to meet at least half way here to be absorbing and appreciating the points that are being communicated.  You have been party to these tradeoffs, usually more than once.  That you only note and keep coming back to the emergency capability is case in point.  This is only one item in the list of tradeoffs that are communicated and not the  only trumping factor.  And still missing the point of this – all communicated in detail already – that if we have a card to play that may prevent or minimize customer downtime we want a strategy that leans to being able to play this card.  Realistically we hope it is not played even once.  Risk mitigation here is critical.  Remember I even qualified this as a couple of existing customers that were dissatisfied with their CLU experience and threatening to break off participation visible to upper management.  One misstep that could be avoided could make the  difference with a lost customer.  Beyond the one example use (not that any are needed to justify), there was another in which the application was mostly externally configurable with spring, but there was one wiring parameter that was not, that we were going to leverage as in-place configuration update to resolve a customer issue preventing CLU use, but because that ended up not externalized we had to purge the data and resend (continued downtime).  Ironic?  It’s this kind of dynamic backing everything you see in the application today.

Keep in mind again the backdrop of velocity/quality/etc. you note that the team has had extremely high velocity and quality with the development practices in place and the use of technology/architecture as is.  As I said we could probably count on 1 hand the number of critical defects that have come out of production or the minimal number of defects raised from development handoff to Quantim, partners.  And, the always best data point of velocity the team completing a brand new CLU vertical w/CAC in two sprints (with other competing CLU work).  Our component has been most successful for on time delivery and quality in a much more stressed velocity environment than you could imagine.  You are looking to institute sweeping/invasive changes to a process and code base that has been extremely successful.  I’ve mentioned these dynamics and these hard examples many times coming back to the data/communication is there.

Some take-aways and tangent dynamics to be preventative of or improve on moving forward-
•	This particular thread should have never re-occurred.  The same points were made as part of validation design debate.  That debate concluded with numerous reasons why XML was a preferred strategy for the problem being solved there.  Those points need to be absorbed, honored, and championed from that point.
•	The validation framework was completed and closed based on an XML configuration strategy.  That should not have been reopened or compromised in future design use (as here).
•	XML in general as bled out with Spring, etc. should not be a continued debate, in particular to our current commitments to it (Spring, validation, jaxb, …).  This is backed by lots of technical, customer, product etc. considerations for hard value to the system.  This needs to be championed at this point and not challenged.
•	Any considerations of changes to the technology, architecture, design, source/code base, and development practice that exists for and by the team/me that is invasive or contradictory needs to be first discussed between us with a more transparent and proactive communication, before blind-sided in design or implementation.  We had tried to setup this dynamic from the beginning of your start for a more collaborative and cohesive technical dynamic and message to the team, but that keeps falling off.
•	Your use of ‘the team thinks’ is overused.  Most tracks we know are not broadly agreed by ‘the team’ – either at all by certain members or to the same level of urgency, so this positioning is over-generalized.  Remember here that I (/matt)  are also team and lead that needs to have buy-in.  Remember we also talked that often  developers often lack the appreciation or awareness of bigger picture dynamics, customer/product, intangibles or lack the same breadth/depth in these collective items or code base.  This isn’t usually lack of visibility to these things, just a skill set that some developers in general struggle with.  This is also a gap for you in analysis in getting up to speed still that we need to strengthen.
•	We believe your positioning of tradeoffs is always driven from developer outward rather than customer or big picture inward.  We appreciate developer convenience and the indirect factors from this, but ultimately we drive but what is most directly of value to parties and application outside the developer bubble.  I like an analogy of a dart board with the developer in the center perhaps.  All the space around are customer, partner, etc. etc.  The arrows go in to what the  developer can do to facilitate those needs or conveniences. If you could more switch this dynamic you’ll be more effective.
•	In order for me to leverage your intended role here I need to have more confidence of the above and that you are championing things I have positioned or would like to position, and in practice there is less an invasive approach to what exists.  This would be great and desired to become a highly collaborative relationship.    

It seems from our onsite discussion 1-1-1, the later onsite discussions or here were more steps back rather than forward. I think we could be very powerful together if we could get some traction on some of these things.  We could sync by call in lieu of risk of another extended thread, but wanted to start in print to get all the thoughts out.

My statement was that it is not a net difference concerning logistic.  XML is so prevalent in every-day development technical life and at our levels it should be nothing but easy to maintain.  This is a dependent interface representation pervasive in CLU as documents exchanged with client applications, development environments (i.e. maven), spring, etc.  There are many camps of people even within the spring world that are still divided with preference toward XML, so though the option  is there doesn’t imply this is an assumed preferred direction.   Keep in mind the bigger picture dynamics and intangibles that make the stronger case for XML here– we’ve exchanged a number of things here (my list is long even beyond the  production use case).  The debate is otherwise more narrowed to development, and though XML does have its benefits too within development, it has benefits out of that space as well.  It is well understood/agreed annotations are prevalent and often the preferred or only sensible approach, and we use them heavily in that regard, such as the web tier.  But, it’s a case-by-case basis requiring applying a more diligent, wider set of dynamics when/where to use them to be most effective.  

We shouldn’t be putting into any of our technical arguments here or moving forward that XML is hard or slowing moving forward.  There are more real things to focus on in those debates.

Again I’ve noted I disagree there is any concerning timing difference here.  Updating XML is easy and well understood.   Central configuration has its benefits for efficiency for other various consumption.  And, remember the production logistic was only one of the benefits of the XML, not the only driver.  This logistic should not be affecting to any schedule pressure.  We’ve managed XML dependencies within our application development from the beginnings of CLU under the height of CLU schedule pressure likely not a level that will be reached again, but already shown to be successful in that context.  There also has been no history of maintainability or correctness here either and not in the camp of the downsides.
This is an option, but a less desired approach, since we’d rather not disable all validation just because one of them has an issue.  This forward looking logistic as one basis for pushing XML was almost a card played recently with the birth date check.  This almost went through to production and would have likely been a critical gating issue for immediate work-around, but I caught the functional disconnect here before it got that far.  In this case it’s the only validation in place currently, so all could have been disabled, but in the future this may have warranted a per-item change.
That’s a matter of opinion.  I don’t believe updating the XML is anything but straight-forward and there are inherent advantages of representation centralized here, as well as updatable in production, so I think it will not be a concerning logistic moving forward, especially that I doubt we will be frequently touching this part of the application, so not an often logistic concern.
The validation tier strategy was based on XML configuration and not annotations.  If I understand the wiki design correctly the intent is a mass change in strategy of the jaxb based import POJOs in order to facilitate specifying the BE null check as an annotation.  This implementation should be consistent with the XML configuration expectation.

If the pojo’s here are still jaxb based, though just not generated with each build, adding annotations to the source wouldn’t work anyway, since a later change to the source POJO would need regeneration and be destructive to the previously updated/annotated source.  Remember you/I discussed the context of these Import* objects being jaxb since the team had desired for these objects to be xml-defined.  They are not generated as justified in the wiki because of any external logistic, but rather just because that’s the practice of how the system manages jaxb based POJO source.  If the intent is also to remove the jaxb nature of these objects (not clear from the wiki) that is something that can be discussed, but not a scope dependency for this US.

Updating XML config should be trivial, consistent with the implementation practice expected in this tier, and avoid rework of POJOs.

Perhaps I missed something in the read of the wiki though…  We’re staying XML-based here, right?
Right, though we would like to do something in the interim as a work-around, such as fail it with that granular specificity before it goes through the engine, since it’s skipping the engine anyway (though we really don’t want to own/duplicate that engine  logic).   I have not heard back from the  engine team if they do anything with the metadata as part of this skipping today, but I  am guessing they don’t.  Either this has to be raised as a defect to fix and CAS has to let the document through with no opportunity for specificity work-around, or todays behavior remains the same and we can reject it earlier w/specificity.
This has been externalCreateDate for all CLU products.  Remember we did some user stories for setting/applying document service date through to the engine via this field.  This was always prescribed as a document service date.  This field is used because it’s at the level of a document.  This is the field the SDK is using to pass the value.  Visit has its own service date and is set via visit level metadata.  The age is computed against this.  I assume the only change is in the job manager to use this directly from sourceDocument rather than sourceDocument#visit.
It’s an issue Alan, and not an exaggeration (full spec), because any design item that was misunderstood or forgotten has in practice circled back to being blamed on the description, usually as ‘not there’, or below an abstract pronoun.

Also, there are other bigger picture dynamics others typically don’t bring to the tradeoffs.  Here, remember I mentioned the description in retro not being practical for this because the audience is much wider than dev.  People like Tim, Karen or even outside our group may reference the US and that kind of (internal) detail makes the US difficult to read and get through the noise by those folks.  Yes, I recommended consideration of other places, but notice you came back here to ‘description’, so this was glossed over.  This is a minor example to note, so more the point takeaway.  There are other things like the more detail (every i and t) we have to cover in planning (and circle to documentation) now extends planning.  However, there were retro’s not too long ago where the broad team had said planning was taking too long and asked they be more streamlined.  The ‘descriptions’ have not been detailed not because we don’t know we can add content (there or elsewhere), but this has never been an issue raised by the  team in the past and they have progressed the sprint w/o this level - and until about this year, the complexity and amount of design for our product dependencies and USs had been magnitudes higher than anything you’ve seen.  I’ve noticed the planning of relatively  straight-forward US a much longer process – a ZA dynamic in particular – yet those tend to be also where more refreshing occurs.  Perhaps this is just some people there being newer, so it may be more a dynamic of getting more breadth depth experience w/the product, architecture, and code base.   Others, especially I and Matt have the most breadth/depth and experience with team member sensitivities that have come and gone, so consider it not only not appropriate to gloss over, but perhaps even to weigh higher.  A lot of the things proposed I think are band-aids to the process that are extending the process due to other underlying dynamics that could be improved – such as people being more engaged in planning.  We have to be careful here since if the sh?t hits the fan again with unrealistic and competing deadlines like in the past we’re less streamlined than before to react to it, as adaptively as we managed it in the past.

Or maybe the true problem at hand is not enough people asking enough questions, or speaking up, or getting engaged in the planning, or just nodding heads, which are all things that Matt has called out in the past, so the root cause may not be based on needing more words in the description.  The description is not meant to be a full functional/design spec of content.
IMO it’s not practical (or the intent of the process) for every i to be dotted and every t to be crossed and all detailed/documented in the US in planning.   The team needs to meet half way on this absorbing / keeping some level of notes and/or understanding carried through.  It’s natural for a lot of that to evolve as the US is picked up and develops.  This is the basis of our discussion in retro last time w/the hesitation of detailed grooming and such, since even fresh US recently planned in detail are forgotten and need refreshers.  We have had a retro item with this a few sprints ago noting even within sprint it was recommended for a refresher in practice.  So dated groomed content is even more vulnerable to that dynamic and suffers team efficiency for rework (or requirements change by time it’s pulled in).
Remember you/I had a separate conversation about this early on in the US.  The values are typical acronym aliases, such as CDA/CCAV/CAD/… like their peers RTF/PDF/TEXT/…  I’d rather that we keep to what we discussed in planning  and iterated in the email thread (i.e. cda/1.1) then.  This was proposed there with the diligence of expectation the name/value space here is viable/extendable to use ‘/’ and it was consistent with our currently employed practice of using ‘/’ in similar use cases, which we pass through plumbing to the engine.  Should be fine to keep consistent w/what we have.
Justin has a pretty good detailed description in the DS that describes that rationale.  Expanding on it, realistically for the pool to go to any concerning limit should only be due to a bug with a leak somewhere I think.  We could put a high ceiling protection just in case, though if that happens likely that runtime will not be usable anyway – i.e. if max is hit and threads need more instances it will be a performance problem – minimally for synchronous APIs.  The traffic around this in a worse case example flow is maybe 50k docs / day = 35 threads / min across 2+ tomcat instances.  Bounding a realistic max brings in some deployment logistics wanting to ensure any environment doesn’t hit the max otherwise performance will be an issue for the flow of traffic waiting, so given the ability to absorb the growth w/o a max (bug aside) we want to be able to let that grow, the memory footprint is low, and creating instances is costly, so we don’t want to  be idling out instances of the pool and having them grow back in and taking the performance hit on the threads that trigger that.
Normally we’d create the US, task them, and see if the teams have the cycles within the sprint to pull it in beyond what is currently left, with this being the last week.  It would be good to overlap with the marshalling USs, which I think Justin said was on common 2.2.2.  Does it make sense for Alan to take a cut at the US/tasks and QA placeholders and we can review after scrum Wed where we’re at?
Thanks for progressing this Alan.  So the cross-app change here I think is relative to the common branch, so probably we need to create linked product user stories for this for CAPD and CAC (muse presumably is covered in scope for the radiology work here, but probably needs additional QA tasks considered for the broader regression of APIs across MUSE document import services).  I think the  current work for capd 2.3.4 (createDate for Cerner) may be relative to this common branch, so would be an immediate sprint dependency?  CAC though doesn’t necessarily have active work that requires this branch, so could be scoped less urgently?
I’ll add to the note about the design dynamic for doing something different that said differently it also means extending the problem unnecessarily.  And the takeaway is not just about the builder here.  Consider the datetime options Justin was working through and he ended up with an attractive solution that was trivial to implement and a very fast approach.  It’s the bigger picture intangibles I keep coming back to – consider the cost/benefit of extending this problem.  Is it worth finding the fastest library, which might be a different library, and we talked about the negative tradeoffs doing this that Justin alluded to also – extending the WAR footprint, extending third-party library dependencies and additional maven management, and additional logistics for third-party licensing tracking for on-premise, keeping with additional library releases and possible API changes, extending the user story time than what was planned for the sprint which had already run over; etc.  Are these tradeoffs and nullifying the work Justin did and the closure he was at here worth the extra design spin/time with negligible performance difference if even a benefit over what he ended up with?  These again are the bigger picture dynamics that I’m trying to strengthen developers with and Justin’s diligence with that was a great sign we want to continue to promote.  Things are not so black/white and just about the best library/code.  You’re terrifically smart and creative, and if you too could put this in your arsenal in lieu otherwise that would be great added value as a designer/developer.
I know she made comments around having separate classes during the review and the card played here.  Keep in mind heather doesn’t have the same kind of depth of object modeling design and general java development in lieu of the database depth, which she openly recognized and said when she started and is looking to get more depth in.  She definitely did not know or have experience with the Builder pattern.  Also, coders, even great coders, in general often struggle to make the leap to considering the broader picture dynamics and general product priorities as part of their decisions with design and implementation.  I’ve been trying to grow the team with these aspects to grow their careers past that point, so sometimes they may not agree with a design path or similar because of not applying all the aspects to the problem.  This is not a ‘team vote’ situation here.
Ah, so I just looked at the IBMbuilder2 and didn’t catch the instanceof’s were using separate classes.  I thought they were delegating to private helpers.  This is really an overcomplicated design for the problem and unnecessary to proliferate the subclassing here.  I understand fully the options and tradeoffs and what the 2nd option brings, and that it is viable, but also its negative aspects.   It’s also something we explicitly discussed in the Fri review as not being a preferred design approach (proliferating sublcassig).  The original builder I think is definitely a preferred end design approach.  Just because there could be a debate whether the fluent interface aspect of it fits perfectly is not enough to discount it, and the code flow around had benefits in the adapter.  At best it’s a 50/50, though I believe it to be much in favor of the builder, but even near 50/50 +/- it’s not worth the design spin and the reimplementation of a mature design and code progress that existed to immediately leverage.  These tactics also cost resource efficiency and add to the logistics of things delaying on the backlog.

The logistic was here too of the team having considered this in the past as a viable TODO approach for addressing the enhancement there and you can ensure if I’m going to propose something like that it’s because it is viable as an attractive approach.  There will always be options to any design.  It’s not practical or productive to be always be looking for alternatives for the sake of just doing something different or a different view of something better.  Your original redirection of putting the proposed design aside and coding an alternative wasn’t driven you said by thinking what was there didn’t fit, so the dynamic of design review freedom wasn’t in play.

We need to figure better dynamics for working through design/implementation moving forward, so we don’t keep clashing here.

Keep in mind a very high volume customer is maybe 40k +/- documents per day, which if my math is right is 27 threads per minute.  So the performance data point is really relative to a much lower concurrency level.

I could see java being faster with lower threads and joda slower, since to be thread safe with a static manner it would have to have some level of synchronization I’d think around it.  Java will begin to get slower when the volume is very high because you have the memory / garbage collection thrashing of all those object instances.  That logistic would never be a factor for us given the above traffic characteristics.  

Also, this is only a concern for synchronous based APIs – i.e. not a concern for  CAC at all.  We could just see what the Cerner  based (i.e. sync) load test shows against the benchmarks we have to see if there is any noticeable difference in those averages.  This is run against a worst case of 25 burst threads.  I’d suspect it would be negligible.

I disagree this is does not fit the problem.  It is viable, and attractive for solving the code enhancement here, and there are benefits of the pattern that are advantageous to the code base and moving forward.  However, the code you attached is also an option.  I don’t have the cycles to debate it and boath are fine, so since the latter exists that’s fine to run with.
Not all the jobs extend from this.  It was originally named and defined w/fields relative to the only job concept of the time which was document extraction.  The metadata update deviation came a bit later and was not relevant to the (extraction) base job.  Thus message type is cross-cutting.  There is also separate base job data dependencies of inbound vs outbound, so there was a separate job response base.

With the builder, at least with setting inbound message any ‘type’ specific logic is no longer applicable.  If you see any such code dependency there it probably doesn’t need to be.

Other thought expanding of the error table as you narrow the design – this part of the schema (purging tables, process) will be very extendable or refactorable, as it’s not directly a product service application dependency, so if we’re guessing what columns to include for application if even 1, or guessing which N columns not to include, we should be able practically reconsider this later.  It may be that we never consider it as there are tradeoffs to moving this management to a separated table – i.e. additional update dependencies, additional read dependencies and extra joins on a view tables, performance implications on some already sensitive services, I/O implications with the disjoint data the DB will have to manage in lieu of the co-located data, some application considerations, …. – options to mitigate some of this, but considerations none-the-less, so an area we not refactor.  One other logistic to consider w/the error handling for the purge focus is if we should consider some alert mechanism (email?), as an error in a table likely would also be benign visibility unless monitored somehow in lieu.
Also BTW the other dependency w/this US I mentioned we forgot to circle back on – the retaining statistic piece.  Looks like it’s the next items up in context, but maybe something to consider as part of the delete orchestration if any impact.  There are options like doing this as a decoupled process, but the dependency w/data delete is we have to address it in that same deliverable (for CAC 14.3), since we don’t want to go out of  the gate with purge that hasn’t addressed the loss of data yet.
As I said, we could change what was agreed in the cross teams in the TRD, but I don’t see a reason to rock that boat.  I  don’t think there ever has been extra meaning per se or at least that we need to over-analyze it in that regard.  This is fairly simplistic in nature.  No coder status on a suggested code is no ReviewerResponse, or nested status.  We don’t need to require the client application for special conditional handling.  If the app XML has a set value, than it is updated with the internal ‘Unset’ else noop.  The Transformer simply maps the presentation XML to the Canonical CLU object model and the adapter owns what actions are actually occurring against the case model state.